{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f762f3da",
   "metadata": {},
   "source": [
    "# CNN Classification in a Nutshell\n",
    "\n",
    "This example demontrates how global average pooling (GAP) can replace a dense layer in CNN based classification while enabling generation and visualization of class activation maps simultaneously. The input image, size 6x6, is divided into 4 quadrants as follows: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "2 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each of the quadrants can have one of the following objects:\n",
    "\n",
    "* empty, class 0\n",
    "* horizontal line, class 1\n",
    "* vertical line, class 2\n",
    "\n",
    "Following example contains a horizontal line in the 0th quadrant and a vertical line in the 3rd quadrant:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Then we create a simple CNN where the convolutional kernel has a shape $(3, 3, 2)$, i.e. it has two $(3, 3)$ kernels. Each of the kernels is intended to capture either horizontal- or vertical features in the input image. After the convolutional layer we have a global average pooling instead of flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71882e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Reduce TF verbosity\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO') # Reduce TF verbosity\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization, MaxPool2D, LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c912e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadrant_center(quadrant):\n",
    "    if quadrant == 0:\n",
    "        return (1, 1)\n",
    "    elif quadrant == 1:\n",
    "        return (1, 4)\n",
    "    elif quadrant == 2:\n",
    "        return (4, 1)\n",
    "    elif quadrant == 3:\n",
    "        return (4, 4)\n",
    "    else:\n",
    "        raise RuntimeError('Quadrant must be between 0..3')\n",
    "\n",
    "def create_image():\n",
    "    '''Creates an image, size (6, 6), that contains 0..4 vertical lines\n",
    "    and 0..4 horizontal lines that are randomly placed at quadrants:\n",
    "    -----\n",
    "    |0|1|\n",
    "    -----\n",
    "    |2|3|\n",
    "    -----\n",
    "    For example,\n",
    "    0 0 0 0 0 0\n",
    "    1 1 1 0 0 0\n",
    "    0 0 0 0 0 0\n",
    "    0 0 0 0 1 0\n",
    "    0 0 0 0 1 0\n",
    "    0 0 0 0 1 0\n",
    "    '''\n",
    "    \n",
    "    nr = np.random.randint(0, 4)\n",
    "    quadrants = random.sample(range(0, 4), nr)\n",
    "    orientation = np.random.randint(1, 3)\n",
    "    \n",
    "    image = np.zeros((6, 6))\n",
    "    y = 0\n",
    "    \n",
    "    for quadrant in quadrants:\n",
    "        (x_, y_) = quadrant_center(quadrant)\n",
    "        y = orientation\n",
    "        \n",
    "        if orientation == 1:\n",
    "            # horizontal line\n",
    "            image[y_, x_-1:x_+2] = 1\n",
    "        elif orientation == 2:\n",
    "            # vertical line\n",
    "            image[y_-1:y_+2, x_,] = 1\n",
    "        else:\n",
    "            raise RuntimeError('Invalid orientation')\n",
    "    \n",
    "    return image, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d7a3e",
   "metadata": {},
   "source": [
    "# Create training- and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0aeb7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (10000, 6, 6)\n",
      "y_train.shape: (10000,)\n",
      "unique values in y_train: [0. 1. 2.]. Number of samples per unique value: [2544 3760 3696]\n"
     ]
    }
   ],
   "source": [
    "nr_training_samples = 10000\n",
    "X_train = np.empty((nr_training_samples, 6, 6))\n",
    "y_train = np.empty((nr_training_samples))\n",
    "\n",
    "X_test = np.empty((100, 6, 6))\n",
    "y_test = np.empty((100))\n",
    "\n",
    "for ind in range(nr_training_samples):\n",
    "    X, y = create_image()\n",
    "    X_train[ind,:] = X\n",
    "    y_train[ind] = y\n",
    "\n",
    "for ind in range(100):\n",
    "    X, y = create_image()\n",
    "    X_test[ind,:] = X\n",
    "    y_test[ind] = y    \n",
    "\n",
    "values, counts = np.unique(y_train, return_counts=True)\n",
    "print(f'X_train.shape: {X_train.shape}')\n",
    "print(f'y_train.shape: {y_train.shape}')\n",
    "print(f'unique values in y_train: {values}. Number of samples per unique value: {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d6673",
   "metadata": {},
   "source": [
    "# Display some X_test and y_test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94752cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test: \n",
      "[[0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "y_test: 2.0\n",
      "----------------\n",
      "X_test: \n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n",
      "y_test: 2.0\n",
      "----------------\n",
      "X_test: \n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "y_test: 1.0\n",
      "----------------\n",
      "X_test: \n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "y_test: 0.0\n",
      "----------------\n",
      "X_test: \n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "y_test: 1.0\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "for ind in range(5):\n",
    "    print(f'X_test: \\n{X_test[ind]}')\n",
    "    print(f'y_test: {y_test[ind]}')\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dbe5cd",
   "metadata": {},
   "source": [
    "# Create a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a693579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 6, 6, 1)]         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 2, 2, 2)           20        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2, 2, 2)          8         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2)                0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37\n",
      "Trainable params: 33\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "i = Input(shape=(6, 6, 1))\n",
    "l1 = Conv2D(2, (3, 3), activation=LeakyReLU(), padding='valid', strides=(3, 3))(i)\n",
    "l2 = BatchNormalization()(l1)\n",
    "l3 = GlobalAveragePooling2D()(l2)\n",
    "l4 = Dense(3, activation='softmax')(l3)\n",
    "model = Model(i, outputs=l4)\n",
    "model_feature_map = Model(model.input, outputs=l1)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Create the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ffe84",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663d2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.6232 - accuracy: 0.8091 - val_loss: 0.6848 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3694 - accuracy: 0.9903 - val_loss: 0.2706 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2513 - accuracy: 0.9907 - val_loss: 0.1708 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1778 - accuracy: 0.9872 - val_loss: 0.1191 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.9958 - val_loss: 0.0846 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0986 - accuracy: 0.9957 - val_loss: 0.0602 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0760 - accuracy: 0.9976 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0596 - accuracy: 0.9973 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0470 - accuracy: 0.9972 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0377 - accuracy: 0.9976 - val_loss: 0.0168 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "r = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a151137",
   "metadata": {},
   "source": [
    "# Show predictions for different inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554228a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "y: 0\n",
      "predicted y: 0\n",
      "----\n",
      "input:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "y: 1\n",
      "predicted y: 1\n",
      "----\n",
      "input:\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n",
      "y: 2\n",
      "predicted y: 2\n",
      "----\n",
      "input:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "y: 1\n",
      "predicted y: 1\n",
      "----\n",
      "input:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "y: 1\n",
      "predicted y: 1\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for ind in range(5):\n",
    "    X, y = create_image()\n",
    "    predicted = model.predict(X.reshape((1, 6, 6)))\n",
    "    print(f'input:\\n{X}')\n",
    "    print(f'y: {y}')\n",
    "    print(f'predicted y: {np.argmax(predicted)}')\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bca75d",
   "metadata": {},
   "source": [
    "# Generate class activation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eeed012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Predicted class:\n",
      "0\n",
      "\n",
      "Classification layer weights:\n",
      "[[ 2.3277636 -1.4843239  1.5353967]\n",
      " [ 1.5944918  1.474136  -2.6543083]]\n",
      "\n",
      "Classification layer weights for predicted class:\n",
      "[2.3277636 1.5944918]\n",
      "\n",
      "Feature map:\n",
      "[[[[0.5412227  0.39143062]\n",
      "   [0.5412227  0.39143062]]\n",
      "\n",
      "  [[0.5412227  0.39143062]\n",
      "   [0.5412227  0.39143062]]]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOWUlEQVR4nO3dfYxldX3H8feHHRUXlIfi4y4KVkKlJoidGBA1RFDxcY3pA7Toamg2baqiMbFYrfgHaUhjDSYazQZ5aFzBuhKlxseAxJoqZVlIZRktiDyMLOyiIlSsQPn2j3O2vY6zD849M/fS3/uVTOaec8+c+53dfc85587dmVQVkv7/22/SA0haGcYuNcLYpUYYu9QIY5caYexSI4z9MSrJJ5P87TLte1uSk5Zj35qc+H32lZfkauBY4OlV9at92P6twJ9X1UuWYZaLgfmq+sDQ+9Z08ci+wpIcAbwUKOANk51GLTH2lfcW4LvAxcD60TuSHJ7k8iQ7k/wkyceSPA/4JHBCkv9Mcl+/7cVJzu1vzyV53ch+ZpLcm+SF/fLnktyd5OdJvpXk9/v1G4A/A97b7/uf+/W3JTmlv/2EJOcnuat/Oz/JE/r7Tkoyn+Q9SXYk2Z7kbbv7xJNcneTcJP+66/GS/E6STUnuT3Jt/8Vw1/YfTXJnf991SV46ct+HkmxO8tkkDyTZmuTYpf6ltMDYV95bgE3926uSPA0gySrgS8DtwBHAGuCyqpoD/gL4TlUdWFUHL7LPS4HTR5ZfBdxbVVv75a8ARwFPBbb2j01Vbexv/32/79cvsu/3A8cDL6C79HgRMHrK/3TgoH7eM4GPJzlkD5//acCb++1/F/gOcBFwKDAHnDOy7bX94x4KfAb4XJL9R+5fB3xu5P4vJHncHh67bVXl2wq9AS8BHgYO65e/D7y7v30CsBOYWeTj3gp8e8G6i4Fz+9vPBR4AVvfLm4AP7maGg+kuIQ5auJ+RbW4DTulv/xB4zch9rwJu62+fBPxydGZgB3D8bh77auD9I8v/AHxlZPn1wA17+PP7GXBsf/tDwHdH7tsP2A68dNJ/z9P65pF9Za0Hvl5V9/bLn+H/TuUPB26vqkd+251W1S10R8XXJ1lN91zAZ6A7Y0hyXpIfJrmfLmSAw/Zx98+kO9vY5fZ+3S4/WTDzg8CBe9jfPSO3f7nI8v9+bH95MNdfftxHdwYxOvedu25U1aPA/ILZNGJm0gO0IskTgT8GViW5u1/9BODg/lrzTuBZSWYWCX5fvmWy61R+P+Cm/gsAwJ/Sne6eQhf6QXRHyOzjvu8Cng1s65ef1a9bVv31+V8DJwPbqurRJKNzQ/cFctf2+wFrV2K2xyqP7CvnjcB/A8fQXYe+AHge8C901/H/Rncael6SA5Lsn+TE/mPvAdYmefwe9n8Z8ErgL+mP6r0nAb8CfgKsBv5uwcfdAzxnD/u9FPhAkqckOQz4IPDpPX2iA3kS8Aj9pU2SDwJPXrDNHyR5U5IZ4F10n+d3V2C2xyRjXznrgYuq6o6qunvXG/AxumfEQ3fN+lzgDrpT0j/pP/YquiPr3Unu/c1dQ1Vtp3uy68XAZ0fu+ke6U+8fAzfxmzF8CjgmyX1JvrDIrs8FtgD/DnyP7gm+c3+Lz3upvkb3xOJ/0M3/X4yctve+SPdn9DO6J/3eVFUPr8Bsj0m+qEaPSUk+BDy3qs6Y9CyPFR7ZpUYYu9QIT+OlRnhklxqxot9nT1ZX9wIuScvjPqoezGL3rPCLag4GNqzsQ0pN2bjbezyNlxph7FIjjF1qhLFLjRgr9iSnJvlBkluSnD3UUJKGt+TY+5+s8nHg1XT/k+v0JMcMNZikYY1zZH8RcEtV3VpVD9H9F8t1w4wlaWjjxL6GX/8vh/P9ul+TZEOSLUm2dD/ERNIkjBP7Yq/S+Y0X2lfVxqqararZ7mcnSJqEcWKfZ+THAuGPBJKm2jixXwscleTI/sclnQZcMcxYkoa25NfGV9UjSd5O9+ODVgEXVtW2vXyYpAkZ6z/CVNWXgS8PNIukZeQr6KRGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGrHk2JMcnuSbSeaSbEty1pCDSRrWOL/Y8RHgPVW1NcmTgOuSfKOqbhpoNkkDWvKRvaq2V9XW/vYDwBywZqjBJA1rkGv2JEcAxwHXDLE/ScMb6/ezAyQ5EPg88K6qun+R+zcAG7qlg8Z9OElLNNaRPcnj6ELfVFWXL7ZNVW2sqtmqmoXV4zycpDGM82x8gE8Bc1X1keFGkrQcxjmynwi8GXh5khv6t9cMNJekgS35mr2qvg1kwFkkLSNfQSc1wtilRhi71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71AhjlxoxduxJViW5PsmXhhhI0vIY4sh+FjA3wH4kLaOxYk+yFngtcMEw40haLuMe2c8H3gs8ursNkmxIsiXJFnhwzIeTtFRLjj3J64AdVXXdnrarqo1VNVtVs7B6qQ8naUzjHNlPBN6Q5DbgMuDlST49yFSSBrfk2KvqfVW1tqqOAE4DrqqqMwabTNKg/D671IiZIXZSVVcDVw+xL0nLwyO71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71Ahjlxph7FIjxoo9ycFJNif5fpK5JCcMNZikYY37ix0/Cny1qv4wyeOB1QPMJGkZLDn2JE8GXga8FaCqHgIeGmYsSUMb5zT+OcBO4KIk1ye5IMkBCzdKsiHJliRb4MExHk7SOMaJfQZ4IfCJqjoO+AVw9sKNqmpjVc1W1axn+dLkjBP7PDBfVdf0y5vp4pc0hZYce1XdDdyZ5Oh+1cnATYNMJWlw4z4b/w5gU/9M/K3A28YfSdJyGCv2qroBmB1mFEnLyVfQSY0wdqkRxi41wtilRhi71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41YqzYk7w7ybYkNya5NMn+Qw0maVhLjj3JGuCdwGxVPR9YBZw21GCShjXuafwM8MQkM8Bq4K7xR5K0HJYce1X9GPgwcAewHfh5VX194XZJNiTZkmQLPLj0SSWNZZzT+EOAdcCRwDOBA5KcsXC7qtpYVbNVNdsd/CVNwjin8acAP6qqnVX1MHA58OJhxpI0tHFivwM4PsnqJAFOBuaGGUvS0Ma5Zr8G2AxsBb7X72vjQHNJGtjMOB9cVecA5ww0i6Rl5CvopEYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5casdfYk1yYZEeSG0fWHZrkG0lu7t8fsrxjShrXvhzZLwZOXbDubODKqjoKuLJfljTF9hp7VX0L+OmC1euAS/rblwBvHHYsSUNb6jX706pqO0D//qnDjSRpOYz1+9n3RZINwIZu6aDlfjhJu7HUI/s9SZ4B0L/fsbsNq2pjVc1W1SysXuLDSRrXUmO/Aljf314PfHGYcSQtl3351tulwHeAo5PMJzkTOA94RZKbgVf0y5Km2F6v2avq9N3cdfLAs0haRr6CTmqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUiFTVyj1YshO4fS+bHQbcuwLj7Cvn2btpm6nleZ5dVU9Z7I4VjX1fJNnS/aqo6eA8ezdtMznP4jyNlxph7FIjpjH2jZMeYAHn2btpm8l5FjF11+ySlsc0HtklLQNjlxoxNbEnOTXJD5LckuTsKZjn8CTfTDKXZFuSsyY9E0CSVUmuT/KlKZjl4CSbk3y//3M6YcLzvLv/u7oxyaVJ9p/ADBcm2ZHkxpF1hyb5RpKb+/eHrPRcMCWxJ1kFfBx4NXAMcHqSYyY7FY8A76mq5wHHA381BTMBnAXMTXqI3keBr1bV7wHHMsG5kqwB3gnMVtXzgVXAaRMY5WLg1AXrzgaurKqjgCv75RU3FbEDLwJuqapbq+oh4DJg3SQHqqrtVbW1v/0A3T/kNZOcKcla4LXABZOco5/lycDLgE8BVNVDVXXfRIeCGeCJSWaA1cBdKz1AVX0L+OmC1euAS/rblwBvXMmZdpmW2NcAd44szzPhsEYlOQI4DrhmwqOcD7wXeHTCcwA8B9gJXNRfVlyQ5IBJDVNVPwY+DNwBbAd+XlVfn9Q8CzytqrZDdxABnjqJIaYl9iyybiq+J5jkQODzwLuq6v4JzvE6YEdVXTepGRaYAV4IfKKqjgN+wYROTwH66+B1wJHAM4EDkpwxqXmm0bTEPg8cPrK8lgmcgi2U5HF0oW+qqssnPM6JwBuS3EZ3mfPyJJ+e4DzzwHxV7Trb2UwX/6ScAvyoqnZW1cPA5cCLJzjPqHuSPAOgf79jEkNMS+zXAkclOTLJ4+meWLlikgMlCd316FxVfWSSswBU1fuqam1VHUH353NVVU3syFVVdwN3Jjm6X3UycNOk5qE7fT8+yer+7+5kpueJzCuA9f3t9cAXJzHEzCQedKGqeiTJ24Gv0T2LemFVbZvwWCcCbwa+l+SGft3fVNWXJzfS1HkHsKn/An0r8LZJDVJV1yTZDGyl+07K9UzgZapJLgVOAg5LMg+cA5wH/FOSM+m+KP3RSs8FvlxWasa0nMZLWmbGLjXC2KVGGLvUCGOXGmHsUiOMXWrE/wDXMs6eakB8OgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract weights of the classification layer\n",
    "W = model.layers[-1].get_weights()[0]\n",
    "\n",
    "# Generate a test image\n",
    "X, y = create_image()\n",
    "\n",
    "# Predict the class\n",
    "prediction = model.predict(X.reshape((1, 6, 6)))\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Generate a feature map\n",
    "feature_map = model_feature_map.predict(X.reshape((1, 6, 6)))\n",
    "\n",
    "# Weights that correspond to the predicted class\n",
    "W_class = W[:,predicted_class]\n",
    "\n",
    "print(f'Input:\\n{X}\\n')\n",
    "print(f'Predicted class:\\n{predicted_class}\\n')\n",
    "print(f'Classification layer weights:\\n{W}\\n')\n",
    "print(f'Classification layer weights for predicted class:\\n{W_class}\\n')\n",
    "print(f'Feature map:\\n{feature_map}\\n')\n",
    "\n",
    "# Create the activation map\n",
    "activation_map = feature_map.dot(W_class)[0]\n",
    "\n",
    "# Upscale the activation map\n",
    "activation_map = sp.ndimage.zoom(activation_map, (6, 6))\n",
    "\n",
    "plt.imshow(X)\n",
    "plt.title('Input')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(activation_map, cmap='jet')\n",
    "plt.title('Activation map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4be96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
