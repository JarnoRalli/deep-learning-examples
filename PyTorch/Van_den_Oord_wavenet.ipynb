{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d27a60c9-d931-45fa-be42-fa9e10be92ab",
   "metadata": {},
   "source": [
    "# Wavenet\n",
    "\n",
    "This is an implementation of a character level NLP model, motivated by [van den Oord et al.](https://arxiv.org/pdf/1609.03499.pdf) paper, based on Andrej Karpathy's [Makemore](https://www.youtube.com/watch?v=t3YJ5hKiMQ0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=6) lectures.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<figure align=\"center\">\n",
    "    <img src=\"./images/van_den_oord_wavenet.png\" width=\"500\">\n",
    "    <figcaption>van den Oord et al.'s wavenet.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<figure align=\"center\">\n",
    "    <img src=\"./images/van_den_oord_wavenet_building_block.png\" width=\"500\">\n",
    "    <figcaption>Building block of the wavenet.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be2693-16ab-4e9a-a61e-8bc67df0bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the seed for the random number generator for repeatability\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916485ec-5ef1-4f79-8379-2317fe4962d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url to the file containing over 30k names\n",
    "url_source = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
    "\n",
    "text = str('')\n",
    "\n",
    "# Read to a variable, line by line\n",
    "for line in urllib.request.urlopen(url_source):\n",
    "    text += line.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441a0d4-1a03-4c77-9215-1cd1e4dbc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the file to lines and show the first 10 lines\n",
    "words = text.splitlines()\n",
    "print(words[:10])\n",
    "print(f\"Total number of words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a4e12-ca29-42a7-8635-ecafcb5b6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocabulary of characters and mappings to/from integers\n",
    "# chars is a set of all the characters found in the text, converted into a list and ordered\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(f\"chars:{chars}\\n\")\n",
    "print(f\"stoi: {stoi}\\n\")\n",
    "print(f\"itos: {itos}\\n\")\n",
    "print(f\"vocabulary size: {vocab_size} letters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c0537-1d9f-4c1f-bbae-35e3f6e8910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the words\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098c759-9c68-48d0-8a53-1bc2c3dee1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "\n",
    "# Context length -> how many characters do we take to predict the next one\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words: List, block_size: int) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"Builds a character dataset for NLP. Outputs are the context (n-number of previous characters) and \n",
    "    the target character.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words : List\n",
    "        A list of words used in learning.\n",
    "    block_size : int\n",
    "        Number of characters in the learning dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.tensor, torch.tensor]\n",
    "        [context, target]\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Iterate through the words\n",
    "    for w in words:\n",
    "        # A neat trick to create a list and initialize it\n",
    "        context = [0] * block_size\n",
    "\n",
    "        # Iterate through the characters of the word that have been added with '.' at the end\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "# Training, validation and test dataset lengths\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "X_training, Y_training = build_dataset(words[:n1], block_size)\n",
    "X_validation, Y_validation = build_dataset(words[n1:n2], block_size)\n",
    "X_test, Y_test = build_dataset(words[n2:], block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1341c4-2735-496b-9b69-0a77517790f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
