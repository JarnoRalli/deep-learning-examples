{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea8ed5b-18ae-491c-8022-c281e8bed20b",
   "metadata": {},
   "source": [
    "# Tokenizer Examples\n",
    "\n",
    ">A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a “Fast” implementation based on the Rust library 🤗 Tokenizers. The “Fast\" implementations allows:\n",
    ">\n",
    "> * a significant speed-up in particular when doing batched tokenization and\n",
    "> * additional methods to map between the original string (character and words) and the token space (e.g. getting the index of the token comprising a given character or the span of characters corresponding to a given token).\n",
    ">\n",
    ">The base classes [PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.39.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) and [PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/v4.39.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) implement the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and “Fast” tokenizers either from a local file or directory or from a pretrained tokenizer provided by the library (downloaded from HuggingFace’s AWS S3 repository). They both rely on [PreTrainedTokenizerBase](https://huggingface.co/docs/transformers/v4.39.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase) that contains the common methods, and [SpecialTokensMixin](https://huggingface.co/docs/transformers/v4.39.0/en/internal/tokenization_utils#transformers.SpecialTokensMixin).\n",
    ">\n",
    ">[PreTrainedTokenizer](https://huggingface.co/docs/transformers/v4.39.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) and [PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/v4.39.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) thus implement the main methods for using all the tokenizers:\n",
    ">\n",
    "> * Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers).\n",
    "> * Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece…).\n",
    "> *Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization.\n",
    "\n",
    "Source: [https://huggingface.co/docs/transformers/main_classes/tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
    "\n",
    "## Special Tokens\n",
    "\n",
    "### Tokens [CLS] and [SEP]\n",
    "\n",
    "BERT has been trained using the format: [CLS] sentence 1 [SEP] sentence 2 [SEP]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7b641f-f6e6-412a-b121-5eb6b7ff5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22497026-b392-4426-8e87-8139d0f24194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer that is compatible with the model \"bert-base-uncased\"\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d7220e-e779-421d-891c-aa18ebab1735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out information regarding the tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79523b58-599b-4ca4-8b4c-e11c799d6e62",
   "metadata": {},
   "source": [
    "### Convert Tokens to IDS and Back to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a03f7ef-a9d1-4be6-8c95-3d377f04eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the string \"hello world\" and print the tokens\n",
    "tokens = tokenizer.tokenize(\"hello world\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8775ae6-1422-4f9b-b810-4e9fba711640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to indices and print the indices\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2cc853-dfe5-4b8e-9edd-deb438d4dbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the indices back to tokens\n",
    "tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c387435-9609-4efa-95bf-c48f15ccf422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world [SEP]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the indices, i.e. convert the ids back to a string\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a157e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 2097, 2175]\n",
      "[1045, 2572, 2183]\n"
     ]
    }
   ],
   "source": [
    "# How are suffixes treated by this tokenizer?\n",
    "# How are the words \"go\" and \"going\" converted into ids?\n",
    "ids1 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I will go\"))\n",
    "print(ids1)\n",
    "\n",
    "ids2 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I am going\"))\n",
    "print(ids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56909a3-857f-4d9f-8763-6c405052cef8",
   "metadata": {},
   "source": [
    "### Encode and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85288de2-c56f-45c2-9b72-6a76d91b4c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 2088, 102]\n"
     ]
    }
   ],
   "source": [
    "# Encode the string. Tokenizes the string and converts it to indices and adds start, end and other tokens\n",
    "ids = tokenizer.encode(\"hello world\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b7d239f-1c9c-4cfd-ac14-7ba1115e2103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6a7d9-fc3b-41c4-8989-9c403e2cfa2f",
   "metadata": {},
   "source": [
    "### Generate Input for The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0963adda-3d5a-4406-80ca-85c6754b3767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(\"hello world\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb2679d4-1396-41ae-ae1d-7e13cafe943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2066, 8870, 1012, 102], [101, 2079, 2017, 2066, 8870, 2205, 1029, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = [\"I like cats.\", \"Do you like cats too?\"]\n",
    "tokenizer(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1540b-9ed6-4d37-8e6c-fcdaead94620",
   "metadata": {},
   "source": [
    "## Sequence Classification Example\n",
    "\n",
    "When we create the sequence classification model, we get a warning saying that the head of the model has not been trained, i.e. the results we get are meaningless that this stage. We need to train the head in order to get meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e80e4027-ceee-48c3-a4b0-b165cda6bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "852db951-1fe7-4f61-84a5-c64bc471b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(\"hello world\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66375db9-ba35-4b53-8e5a-10c07ea7c7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3159, -0.0049]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**model_inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29107fb9-e049-4876-8773-4acf41b862c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da40d088-ecbf-49dd-b554-7b91d0f52f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3323,  0.3484, -0.2075]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**model_inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5ac45e6-d78f-48bc-afae-4f4e4d843c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3323,  0.3484, -0.2075]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b89be15a-b85f-4038-850a-2b54975c19cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33225596,  0.34839675, -0.20746157]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc4017dd-97b8-4c35-810e-616ca8adac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For batched inputs, depending on the version of 'transformers' library you are using, you might need to add the\n",
    "# parameters padding=True and truncation=True if the output is in a tensor format. Tensors need to have same length.\n",
    "text_input = [\"I like cats.\", \"Do you like cats too?\"]\n",
    "model_inputs = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "697ee364-9bd5-42d7-8d1e-6901f7a02b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0,    0],\n",
      "        [ 101, 2079, 2017, 2066, 8870, 2205, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# print the model_inputs\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "817a5dd8-6e9d-47a3-8470-dab29226d536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2066, 8870, 1012,  102,    0,    0],\n",
      "        [ 101, 2079, 2017, 2066, 8870, 2205, 1029,  102]])\n"
     ]
    }
   ],
   "source": [
    "# print the input_ids\n",
    "print(model_inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be7f9e96-3a1c-42b3-a1e3-b65ef11a8b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# print the attention mask, i.e. which tokens are used\n",
    "print(model_inputs.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dd08548-d1d3-4fc1-8f79-e3c407f00917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do inference using the batch model_input\n",
    "outputs = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50822e8b-dce9-470e-b633-016e20b64e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3043,  0.4186, -0.2840],\n",
      "        [-0.3122,  0.4685, -0.3278]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
