{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac0d105-6a1f-4a4b-93b9-dbe4d38a4bca",
   "metadata": {},
   "source": [
    "# Fine-tuning Sentiment Analysis\n",
    "\n",
    "In this notebook we take a pre-trained distilbert model that has an un-trained head for sentiment analysis. In other words, the head needs to be trained on given examples so that the model learns to catagorize sequences of text into either 'positive' or 'negative' categories.\n",
    "\n",
    ">This model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was introduced in this [paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found here. This model is uncased: it does not make a difference between english and English.\n",
    "\n",
    "The model is trained with the following dataset (loaded from Huggingface)\n",
    "\n",
    "* [GLUE](https://gluebenchmark.com/) (General Language Understanding Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c5a1f2-e023-4fd3-9953-0c349aa73a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "from typing import Tuple, Dict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b181d9-017b-4ffb-86bc-4c90f23a9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLUE stands for General Language Understanding Evaluation\n",
    "# Loads sst2 (sentiment analysis) task data from the GLUE dataset\n",
    "raw_dataset = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141f39a0-728f-4ca7-b3ff-9f5617a3b9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show contents of the dataset. In essence we have the following:\n",
    "# - training datast\n",
    "# - validation dataset\n",
    "# - test dataset\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25523c6-bb7d-405e-9b80-35f4c49f3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model to be used and the corresponding tokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ec119a-be82-4e33-a83b-3e88988cc59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrases: ['hide new secretions from the parental units ', 'contains no wit , only labored gags ', 'that loves its characters and communicates something rather beautiful about human nature ']:\n",
      "token ids:{'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102], [101, 3397, 2053, 15966, 1010, 2069, 4450, 2098, 18201, 2015, 102], [101, 2008, 7459, 2049, 3494, 1998, 10639, 2015, 2242, 2738, 3376, 2055, 2529, 3267, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenize a few samples\n",
    "tokenized_samples = tokenizer(raw_dataset['train'][0:3]['sentence'])\n",
    "print(f\"phrases: {raw_dataset['train'][0:3]['sentence']}:\\ntoken ids:{tokenized_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2d08a3-5211-44bc-abea-33874803a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer function to apply tokenization to all of the datasets\n",
    "def tokenizer_fn(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e2a2a39-cf48-48c9-b256-e6b58ca41f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_dataset.map(tokenizer_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5a6594-5d8d-4ce6-a6e7-6a695edad2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters. Train only for 1 epoch.\n",
    "training_parameters = TrainingArguments(\n",
    "    'distil-bert-trainer',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf78e01-82d0-44de-acf6-c835d190863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create the model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e4fd88-a8df-45e0-90ea-e31318460421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the type of the model\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b9ac8f4-730a-4338-9348-b24ab41827ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the model layers\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f232a97d-fc88-4a16-9788-53b56cc4c3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print summary of the model\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f69cae7-0d88-4d2e-853b-3491b01e953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the weights to a list so that we can compare the weights\n",
    "params_before_training = []\n",
    "for name, p in model.named_parameters():\n",
    "    params_before_training.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6bef263-f1f4-40ee-b8a7-a3fb3c34f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metric object for evaluating the performance\n",
    "metric = evaluate.load(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b0f651-586e-4161-9980-68774e551db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how the metric object works\n",
    "metric.compute(predictions=[1,0,1], references=[1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b37109d4-fb68-4686-acc6-29bcb4d36948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a compute_metrics function that is passed to the Trainer-object\n",
    "def compute_metrics(logits_and_labels: Tuple) -> Dict:\n",
    "    \"\"\"Computes accuracy metrics based on the given logits from the network and the correct labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits_and_labels: Tuple\n",
    "        Contains the logits from the network and the actual labels\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    Dict\n",
    "        A dictionary in the format {'accuracy': val}\n",
    "    \"\"\"\n",
    "    \n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da4800f-650a-4e89-9558-4c69c42c17b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarno/miniconda3/envs/huggingface_gpu/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Trainer object used for training the model\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_parameters,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3df05c51-0a9b-47aa-964b-d307ae282a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cce08f5ea9a4605abcc46d8ffc6ae13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4153, 'grad_norm': 1.6516419649124146, 'learning_rate': 4.7030526190759e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3501, 'grad_norm': 16.05232810974121, 'learning_rate': 4.4061052381518e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3311, 'grad_norm': 2.726660966873169, 'learning_rate': 4.109157857227699e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3078, 'grad_norm': 10.306052207946777, 'learning_rate': 3.812210476303599e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2751, 'grad_norm': 5.610777378082275, 'learning_rate': 3.515263095379499e-05, 'epoch': 0.3}\n",
      "{'loss': 0.2768, 'grad_norm': 0.42856350541114807, 'learning_rate': 3.218315714455399e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2601, 'grad_norm': 0.3930196166038513, 'learning_rate': 2.9213683335312986e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2479, 'grad_norm': 50.17327117919922, 'learning_rate': 2.6244209526071984e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2418, 'grad_norm': 0.25097548961639404, 'learning_rate': 2.3274735716830978e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2291, 'grad_norm': 6.732576847076416, 'learning_rate': 2.0305261907589976e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2379, 'grad_norm': 3.0131266117095947, 'learning_rate': 1.7335788098348973e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2407, 'grad_norm': 7.579282760620117, 'learning_rate': 1.4366314289107971e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2173, 'grad_norm': 17.697776794433594, 'learning_rate': 1.1396840479866969e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2265, 'grad_norm': 9.460827827453613, 'learning_rate': 8.427366670625965e-06, 'epoch': 0.83}\n",
      "{'loss': 0.2169, 'grad_norm': 26.051774978637695, 'learning_rate': 5.457892861384962e-06, 'epoch': 0.89}\n",
      "{'loss': 0.197, 'grad_norm': 39.195701599121094, 'learning_rate': 2.4884190521439603e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a97bb3fd054d14b9aac390f52e21fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3753179907798767, 'eval_accuracy': 0.893348623853211, 'eval_runtime': 1.5762, 'eval_samples_per_second': 553.221, 'eval_steps_per_second': 69.153, 'epoch': 1.0}\n",
      "{'train_runtime': 531.8828, 'train_samples_per_second': 126.624, 'train_steps_per_second': 15.829, 'train_loss': 0.2635245966024928, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8419, training_loss=0.2635245966024928, metrics={'train_runtime': 531.8828, 'train_samples_per_second': 126.624, 'train_steps_per_second': 15.829, 'train_loss': 0.2635245966024928, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b27d7db4-2077-411f-b0b5-f03401a23812",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"trained_distilbert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9df92e0a-6244-4c01-8838-cc0c867e7332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is execute in: GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Model is execute in: GPU\")\n",
    "    sequence_classifier = pipeline(\"text-classification\", model=\"trained_distilbert_model\", device=torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"Model is execute in: CPU\")\n",
    "    sequence_classifier = pipeline(\"text-classification\", model=\"trained_distilbert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f28cb89-08b1-4bd1-9db5-7a6b4cd76fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment label to text\n",
    "label2sentiment={\"LABEL_0\": \"negative\", \"LABEL_1\": \"positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51c87478-b48c-4983-bedc-6bd5af0403dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie sucks big time! -> sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Test the new model\n",
    "phrase = \"This movie sucks big time!\"\n",
    "sentiment = sequence_classifier(phrase)\n",
    "print(f\"{phrase} -> sentiment: {label2sentiment[sentiment[0]['label']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "796264e8-e5c5-461e-929c-2599d29abcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is fantastic! -> sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Test the new model\n",
    "phrase = \"This movie is fantastic!\"\n",
    "sentiment = sequence_classifier(phrase)\n",
    "print(f\"{phrase} -> sentiment: {label2sentiment[sentiment[0]['label']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7f26136-1311-402b-bbcb-4e44b7fb1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model configuration\n",
    "model_config_file=\"trained_distilbert_model/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f2b67f0-5b1e-4c07-a644-69795782185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the config-file of the trained model\n",
    "with open(model_config_file) as f:\n",
    "    model_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21f1f434-e677-418e-b8ac-c8498e332f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_name_or_path: distilbert-base-uncased\n",
      "activation: gelu\n",
      "architectures: ['DistilBertForSequenceClassification']\n",
      "attention_dropout: 0.1\n",
      "dim: 768\n",
      "dropout: 0.1\n",
      "hidden_dim: 3072\n",
      "initializer_range: 0.02\n",
      "max_position_embeddings: 512\n",
      "model_type: distilbert\n",
      "n_heads: 12\n",
      "n_layers: 6\n",
      "pad_token_id: 0\n",
      "problem_type: single_label_classification\n",
      "qa_dropout: 0.1\n",
      "seq_classif_dropout: 0.2\n",
      "sinusoidal_pos_embds: False\n",
      "tie_weights_: True\n",
      "torch_dtype: float32\n",
      "transformers_version: 4.39.1\n",
      "vocab_size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Show the model parameters\n",
    "for key in model_config:\n",
    "    print(f\"{key}: {model_config[key]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
